{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import dirname\n",
    "\n",
    "\n",
    "def replace_nan_with_row_mean(a):\n",
    "    out = np.where(np.isnan(a), ma.array(a, mask=np.isnan(a)).mean(axis=1)[:, np.newaxis], a)\n",
    "    return np.float32(out)\n",
    "\n",
    "def replace_nan_with_near_value(a):\n",
    "    mask = np.isnan(a)\n",
    "    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
    "    np.maximum.accumulate(idx,axis=1, out=idx)\n",
    "    out = a[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return np.float32(out)\n",
    "\n",
    "def set_nan_to_zero(a):\n",
    "    where_are_NaNs = np.isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSC_data_loader(dataset_path,dataset_name):\n",
    "    Train_dataset = np.loadtxt(\n",
    "        dataset_path + '/' + dataset_name + '/' + dataset_name + '_TRAIN.tsv')\n",
    "    Test_dataset = np.loadtxt(\n",
    "        dataset_path + '/' + dataset_name + '/' + dataset_name + '_TEST.tsv')\n",
    "    Train_dataset = Train_dataset.astype(np.float32)\n",
    "    Test_dataset = Test_dataset.astype(np.float32)\n",
    "\n",
    "    X_train = Train_dataset[:, 1:]\n",
    "    y_train = Train_dataset[:, 0:1]\n",
    "\n",
    "    X_test = Test_dataset[:, 1:]\n",
    "    y_test = Test_dataset[:, 0:1]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.squeeze(y_train, axis=1))\n",
    "    y_train = le.transform(np.squeeze(y_train, axis=1))\n",
    "    y_test = le.transform(np.squeeze(y_test, axis=1))\n",
    "    return replace_nan_with_row_mean(X_train), y_train, replace_nan_with_row_mean(X_test), y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1422, 3, 180)\n",
      "(1422,)\n",
      "(1436, 3, 182)\n",
      "(1436,)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "def fill_out_with_Nan(data,max_length):\n",
    "    #via this it can works on more dimensional array\n",
    "    pad_length = max_length-data.shape[-1]\n",
    "    if pad_length == 0:\n",
    "        return data\n",
    "    else:\n",
    "        pad_shape = list(data.shape[:-1])\n",
    "        pad_shape.append(pad_length)\n",
    "        Nan_pad = np.empty(pad_shape)*np.nan\n",
    "        return np.concatenate((data, Nan_pad), axis=-1)\n",
    "    \n",
    "\n",
    "def get_label_dict(file_path):\n",
    "    label_dict ={}\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if '@classLabel' in line:\n",
    "                label_list = line.replace('\\n','').split(' ')[2:]\n",
    "                for i in range(len(label_list)):\n",
    "                    label_dict[label_list[i]] = i \n",
    "                \n",
    "                break\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def get_data_and_label_from_ts_file(file_path,label_dict):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        Start_reading_data = False\n",
    "        Label_list = []\n",
    "        Data_list = []\n",
    "        max_length = 0\n",
    "        for line in lines:\n",
    "            if Start_reading_data == False:\n",
    "                if '@data'in line:\n",
    "                    Start_reading_data = True\n",
    "            else:\n",
    "                temp = line.split(':')\n",
    "                Label_list.append(label_dict[temp[-1].replace('\\n','')])\n",
    "                data_tuple= [np.expand_dims(np.fromstring(channel, sep=','), axis=0) for channel in temp[:-1]]\n",
    "                max_channel_length = 0\n",
    "                for channel_data in data_tuple:\n",
    "                    if channel_data.shape[-1]>max_channel_length:\n",
    "                        max_channel_length = channel_data.shape[-1]\n",
    "                data_tuple = [fill_out_with_Nan(data,max_channel_length) for data in data_tuple]\n",
    "                data = np.expand_dims(np.concatenate(data_tuple, axis=0), axis=0)\n",
    "                Data_list.append(data)\n",
    "                if max_channel_length>max_length:\n",
    "                    max_length = max_channel_length\n",
    "        \n",
    "        Data_list = [fill_out_with_Nan(data,max_length) for data in Data_list]\n",
    "        X =  np.concatenate(Data_list, axis=0)\n",
    "        Y =  np.asarray(Label_list)\n",
    "        \n",
    "        return np.float32(X), Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def TSC_multivariate_data_loader(dataset_path, dataset_name):\n",
    "    \n",
    "    Train_dataset_path = dataset_path + '/' + dataset_name + '/' + dataset_name + '_TRAIN.ts'\n",
    "    Test_dataset_path = dataset_path + '/' + dataset_name + '/' + dataset_name + '_TEST.ts'\n",
    "    label_dict = get_label_dict(Train_dataset_path)\n",
    "    X_train, y_train = get_data_and_label_from_ts_file(Train_dataset_path,label_dict)\n",
    "    X_test, y_test = get_data_and_label_from_ts_file(Test_dataset_path,label_dict)\n",
    "    \n",
    "    return set_nan_to_zero(X_train), y_train, set_nan_to_zero(X_test), y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = 'CharacterTrajectories'\n",
    "dataset_path = dirname(\"./Example_Datasets/UEAArchive_2018/\")\n",
    "#TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "\n",
    "X_train, y_train, X_test, y_test = TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_condition(iepoch,print_result_every_x_epoch):\n",
    "    if (iepoch + 1) % print_result_every_x_epoch == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    predict_list = np.array([])\n",
    "    label_list = np.array([])\n",
    "    for sample in dataloader:\n",
    "        y_predict = model(sample[0])\n",
    "        y_predict = y_predict.detach().cpu().numpy()\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "        predict_list = np.concatenate((predict_list, y_predict), axis=0)\n",
    "        label_list = np.concatenate((label_list, sample[1].detach().cpu().numpy()), axis=0)\n",
    "    acc = accuracy_score(predict_list, label_list)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def save_to_log(sentence, Result_log_folder, dataset_name):\n",
    "    father_path = Result_log_folder + dataset_name\n",
    "    if not os.path.exists(father_path):\n",
    "        os.makedirs(father_path)\n",
    "    path = father_path + '/' + dataset_name + '_.txt'\n",
    "    print(path)\n",
    "    with open(path, \"a\") as myfile:\n",
    "        myfile.write(sentence + '\\n')\n",
    "        \n",
    "        \n",
    "def get_Prime_number_in_a_range(start, end):\n",
    "    Prime_list = []\n",
    "    for val in range(start, end + 1): \n",
    "        prime_or_not = True\n",
    "        for n in range(2, val):\n",
    "            if (val % n) == 0:\n",
    "                prime_or_not = False\n",
    "                break\n",
    "        if prime_or_not:\n",
    "            Prime_list.append(val)\n",
    "    return Prime_list\n",
    "\n",
    "\n",
    "def get_out_channel_number(paramenter_layer, in_channel, prime_list):\n",
    "    out_channel_expect = int(paramenter_layer/(in_channel*sum(prime_list)))\n",
    "    return out_channel_expect\n",
    "\n",
    "def generate_layer_parameter_list(start,end,paramenter_number_of_layer_list, in_channel = 1):\n",
    "    prime_list = get_Prime_number_in_a_range(start, end)\n",
    "    if prime_list == []:\n",
    "        print('start = ',start, 'which is larger than end = ', end)\n",
    "    paramenter_number_of_layer_list[0] =  paramenter_number_of_layer_list[0]*in_channel\n",
    "    input_in_channel = in_channel\n",
    "    layer_parameter_list = []\n",
    "    for paramenter_number_of_layer in paramenter_number_of_layer_list:\n",
    "        out_channel = get_out_channel_number(paramenter_number_of_layer, in_channel, prime_list)\n",
    "        \n",
    "        tuples_in_layer= []\n",
    "        for prime in prime_list:\n",
    "            tuples_in_layer.append((in_channel,out_channel,prime))\n",
    "        in_channel =  len(prime_list)*out_channel\n",
    "        \n",
    "        layer_parameter_list.append(tuples_in_layer)\n",
    "    \n",
    "    tuples_in_layer_last = []\n",
    "    first_out_channel = len(prime_list)*get_out_channel_number(paramenter_number_of_layer_list[0], input_in_channel, prime_list)\n",
    "    tuples_in_layer_last.append((in_channel,first_out_channel,start))\n",
    "    tuples_in_layer_last.append((in_channel,first_out_channel,start+1))\n",
    "    layer_parameter_list.append(tuples_in_layer_last)\n",
    "    return layer_parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def calculate_mask_index(kernel_length_now,largest_kernel_lenght):\n",
    "    right_zero_mast_length = math.ceil((largest_kernel_lenght-1)/2)-math.ceil((kernel_length_now-1)/2)\n",
    "    left_zero_mask_length = largest_kernel_lenght - kernel_length_now - right_zero_mast_length\n",
    "    return left_zero_mask_length, left_zero_mask_length+ kernel_length_now\n",
    "\n",
    "def creat_mask(number_of_input_channel,number_of_output_channel, kernel_length_now, largest_kernel_lenght):\n",
    "    ind_left, ind_right= calculate_mask_index(kernel_length_now,largest_kernel_lenght)\n",
    "    mask = np.ones((number_of_input_channel,number_of_output_channel,largest_kernel_lenght))\n",
    "    mask[:,:,0:ind_left]=0\n",
    "    mask[:,:,ind_right:]=0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def creak_layer_mask(layer_parameter_list):\n",
    "    largest_kernel_lenght = layer_parameter_list[-1][-1]\n",
    "    mask_list = []\n",
    "    init_weight_list = []\n",
    "    bias_list = []\n",
    "    for i in layer_parameter_list:\n",
    "        conv = torch.nn.Conv1d(in_channels=i[0], out_channels=i[1], kernel_size=i[2])\n",
    "        ind_l,ind_r= calculate_mask_index(i[2],largest_kernel_lenght)\n",
    "        big_weight = np.zeros((i[1],i[0],largest_kernel_lenght))\n",
    "        big_weight[:,:,ind_l:ind_r]= conv.weight.detach().numpy()\n",
    "        \n",
    "        bias_list.append(conv.bias.detach().numpy())\n",
    "        init_weight_list.append(big_weight)\n",
    "        \n",
    "        mask = creat_mask(i[1],i[0],i[2], largest_kernel_lenght)\n",
    "        mask_list.append(mask)\n",
    "        \n",
    "    mask = np.concatenate(mask_list, axis=0)\n",
    "    init_weight = np.concatenate(init_weight_list, axis=0)\n",
    "    init_bias = np.concatenate(bias_list, axis=0)\n",
    "    return mask.astype(np.float32), init_weight.astype(np.float32), init_bias.astype(np.float32)\n",
    "\n",
    "    \n",
    "class build_layer_with_layer_parameter(nn.Module):\n",
    "    def __init__(self,layer_parameters):\n",
    "        super(build_layer_with_layer_parameter, self).__init__()\n",
    "\n",
    "        os_mask, init_weight, init_bias= creak_layer_mask(layer_parameters)\n",
    "        \n",
    "        \n",
    "        in_channels = os_mask.shape[1] \n",
    "        out_channels = os_mask.shape[0] \n",
    "        max_kernel_size = os_mask.shape[-1]\n",
    "\n",
    "        self.weight_mask = nn.Parameter(torch.from_numpy(os_mask),requires_grad=False)\n",
    "        \n",
    "        self.padding = nn.ConstantPad1d((int((max_kernel_size-1)/2), int(max_kernel_size/2)), 0)\n",
    "         \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=max_kernel_size)\n",
    "        self.conv1d.weight = nn.Parameter(torch.from_numpy(init_weight),requires_grad=True)\n",
    "        self.conv1d.bias =  nn.Parameter(torch.from_numpy(init_bias),requires_grad=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(num_features=out_channels)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.conv1d.weight.data = self.conv1d.weight*self.weight_mask\n",
    "        #self.conv1d.weight.data.mul_(self.weight_mask)\n",
    "        result_1 = self.padding(X)\n",
    "        result_2 = self.conv1d(result_1)\n",
    "        result_3 = self.bn(result_2)\n",
    "        result = F.relu(result_3)\n",
    "        return result    \n",
    "    \n",
    "    \n",
    "class OS_CNN_block(nn.Module):\n",
    "    def __init__(self,layer_parameter_list,n_class,squeeze_layer = True):\n",
    "        super(OS_CNN_block, self).__init__()\n",
    "        self.squeeze_layer = squeeze_layer\n",
    "        self.layer_parameter_list = layer_parameter_list\n",
    "        self.layer_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(layer_parameter_list)):\n",
    "            layer = build_layer_with_layer_parameter(layer_parameter_list[i])\n",
    "            self.layer_list.append(layer)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layer_list)\n",
    "            \n",
    "        self.averagepool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        out_put_channel_numebr = 0\n",
    "        for final_layer_parameters in layer_parameter_list[-1]:\n",
    "            out_put_channel_numebr = out_put_channel_numebr+ final_layer_parameters[1] \n",
    "            \n",
    "        self.hidden = nn.Linear(out_put_channel_numebr, n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.net(X)\n",
    "        \n",
    "        if self.squeeze_layer:\n",
    "            X = self.averagepool(X)\n",
    "            X = X.squeeze_(-1)\n",
    "            X = self.hidden(X)\n",
    "        return X\n",
    "\n",
    "def check_channel_limit(os_block_layer_parameter_list,n_input_channel,mid_channel_limit): \n",
    "    out_channel_each = 0\n",
    "    for conv_in in os_block_layer_parameter_list[-1]:\n",
    "        out_channel_each = out_channel_each + conv_in[1]\n",
    "    total_temp_channel = n_input_channel*out_channel_each\n",
    "    if total_temp_channel<=mid_channel_limit:\n",
    "        return os_block_layer_parameter_list\n",
    "    else:\n",
    "        \n",
    "        temp_channel_each = max(int(mid_channel_limit/(n_input_channel*len(os_block_layer_parameter_list[-1]))),1)\n",
    "        for i in range(len(os_block_layer_parameter_list[-1])):\n",
    "            os_block_layer_parameter_list[-1][i]= (os_block_layer_parameter_list[-1][i][0],\n",
    "                                                   temp_channel_each,\n",
    "                                                  os_block_layer_parameter_list[-1][i][2])\n",
    "        print('reshape temp channel from ',total_temp_channel,' to ',n_input_channel,' * ',temp_channel_each,)\n",
    "        return os_block_layer_parameter_list\n",
    "\n",
    "    \n",
    "class OS_CNN(nn.Module):\n",
    "    def __init__(self, layer_parameter_list, n_class, n_input_channel,squeeze_layer = True):\n",
    "        super(OS_CNN, self).__init__()\n",
    "        \n",
    "        self.mid_channel_limit = 1000\n",
    "        self.squeeze_layer = squeeze_layer\n",
    "        self.layer_parameter_list = layer_parameter_list\n",
    "        self.OS_block_list = nn.ModuleList()\n",
    "        \n",
    "        os_block_layer_parameter_list = copy.deepcopy(layer_parameter_list[:-1])\n",
    "        os_block_layer_parameter_list = check_channel_limit(os_block_layer_parameter_list,n_input_channel,self.mid_channel_limit)\n",
    "        print('os_block_layer_parameter_list is :',os_block_layer_parameter_list)\n",
    "        for nth in range(n_input_channel):\n",
    "            torch_OS_CNN_block = OS_CNN_block(os_block_layer_parameter_list,n_class, False)\n",
    "            self.OS_block_list.append(torch_OS_CNN_block)\n",
    "        \n",
    "        rf_size = layer_parameter_list[0][-1][-1]\n",
    "        in_channel_we_want= len(layer_parameter_list[1])*os_block_layer_parameter_list[-1][-1][1]*n_input_channel\n",
    "        print('in_channel_we_want is :', in_channel_we_want)\n",
    "       \n",
    "        layer_parameter_list = generate_layer_parameter_list(1,rf_size+1,[8*128, (5*128*256 + 2*256*128)/2],in_channel = in_channel_we_want)\n",
    "        \n",
    "        self.averagepool = nn.AdaptiveAvgPool1d(1) \n",
    "        print(layer_parameter_list)\n",
    "        self.OS_net =  OS_CNN_block(layer_parameter_list,n_class, True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        OS_block_result_list = []\n",
    "        for i_th_channel, OS_block in enumerate(self.OS_block_list):\n",
    "            OS_block_result = OS_block(X[:,i_th_channel:i_th_channel+1,:])\n",
    "            OS_block_result_list.append(OS_block_result)\n",
    "        result = F.relu(torch.cat(tuple(OS_block_result_list), 1)) \n",
    "        \n",
    "        result = self.OS_net(result)\n",
    "        return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import dirname\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "class OS_CNN_easy_use():\n",
    "    \n",
    "    def __init__(self,Result_log_folder, \n",
    "                 dataset_name, \n",
    "                 device, \n",
    "                 Max_kernel_size = 89, \n",
    "                 paramenter_number_of_layer_list = [8*128, 5*128*256 + 2*256*128], \n",
    "                 max_epoch = 2000, \n",
    "                 batch_size=16,\n",
    "                 print_result_every_x_epoch = 50,\n",
    "                 start_kernel_size = 1,\n",
    "                 quarter_or_half = 4\n",
    "                ):\n",
    "        \n",
    "        super(OS_CNN_easy_use, self).__init__()\n",
    "        \n",
    "        if not os.path.exists(Result_log_folder +dataset_name+'/'):\n",
    "            os.makedirs(Result_log_folder +dataset_name+'/')\n",
    "        Initial_model_path = Result_log_folder +dataset_name+'/'+dataset_name+'initial_model'\n",
    "        model_save_path = Result_log_folder +dataset_name+'/'+dataset_name+'Best_model'\n",
    "        \n",
    "\n",
    "        self.Result_log_folder = Result_log_folder\n",
    "        self.dataset_name = dataset_name        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.Initial_model_path = Initial_model_path\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        \n",
    "        self.Max_kernel_size = Max_kernel_size\n",
    "        self.paramenter_number_of_layer_list = paramenter_number_of_layer_list\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.print_result_every_x_epoch = print_result_every_x_epoch\n",
    "        self.quarter_or_half = quarter_or_half\n",
    "        \n",
    "        self.OS_CNN = None\n",
    "        \n",
    "        \n",
    "        self.start_kernel_size = start_kernel_size\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "\n",
    "        print('code is running on ',self.device)\n",
    "        \n",
    "        # covert numpy to pytorch tensor and put into gpu\n",
    "        X_train = torch.from_numpy(X_train)\n",
    "        X_train.requires_grad = False\n",
    "        if len(X_train.shape) ==3:\n",
    "            X_train = X_train.to(self.device)\n",
    "        else:\n",
    "            X_train = X_train.unsqueeze_(1).to(self.device)\n",
    "        y_train = torch.from_numpy(y_train).to(self.device)\n",
    "        n_input_channel = X_train.shape[1]\n",
    "        \n",
    "        X_test = torch.from_numpy(X_val)\n",
    "        X_test.requires_grad = False\n",
    "        if len(X_test.shape) ==3:\n",
    "            X_test = X_test.to(self.device)\n",
    "        else:\n",
    "            X_test = X_test.unsqueeze_(1).to(self.device)\n",
    "        y_test = torch.from_numpy(y_val).to(self.device)\n",
    "        \n",
    "        \n",
    "        input_shape = X_train.shape[-1]\n",
    "        n_class = max(y_train) + 1\n",
    "        receptive_field_shape= min(int(X_train.shape[-1]/self.quarter_or_half),self.Max_kernel_size)\n",
    "        \n",
    "        # generate parameter list\n",
    "        layer_parameter_list = generate_layer_parameter_list(self.start_kernel_size,receptive_field_shape,self.paramenter_number_of_layer_list,in_channel = 1)\n",
    "        print(layer_parameter_list)\n",
    "        torch_OS_CNN = OS_CNN(layer_parameter_list, n_class.item(),n_input_channel, True).to(self.device)\n",
    "        \n",
    "        # save_initial_weight\n",
    "        torch.save(torch_OS_CNN.state_dict(), self.Initial_model_path)\n",
    "        \n",
    "        \n",
    "        # loss, optimizer, scheduler\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(torch_OS_CNN.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=200, min_lr=0.0001)\n",
    "        \n",
    "        # build dataloader\n",
    "        print(max(int(min(X_train.shape[0] / 10, self.batch_size)),2))\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=max(int(min(X_train.shape[0] / 10, self.batch_size)),2), shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=max(int(min(X_train.shape[0] / 10, self.batch_size)),2), shuffle=False)\n",
    "        \n",
    "        \n",
    "        torch_OS_CNN.train()   \n",
    "        \n",
    "        for i in range(self.max_epoch):\n",
    "            for sample in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_predict = torch_OS_CNN(sample[0])\n",
    "                output = criterion(y_predict, sample[1])\n",
    "                output.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step(output)\n",
    "            \n",
    "            if eval_condition(i,self.print_result_every_x_epoch):\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print('epoch =',i, 'lr = ', param_group['lr'])\n",
    "                torch_OS_CNN.eval()\n",
    "                acc_train = eval_model(torch_OS_CNN, train_loader)\n",
    "                acc_test = eval_model(torch_OS_CNN, test_loader)\n",
    "                torch_OS_CNN.train()\n",
    "                print('train_acc=\\t', acc_train, '\\t test_acc=\\t', acc_test, '\\t loss=\\t', output.item())\n",
    "                sentence = 'train_acc=\\t'+str(acc_train)+ '\\t test_acc=\\t'+str(acc_test) \n",
    "                print('log saved at:')\n",
    "                save_to_log(sentence,self.Result_log_folder, self.dataset_name)\n",
    "                torch.save(torch_OS_CNN.state_dict(), self.model_save_path)\n",
    "         \n",
    "        torch.save(torch_OS_CNN.state_dict(), self.model_save_path)\n",
    "        self.OS_CNN = torch_OS_CNN\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        X_test = torch.from_numpy(X_test)\n",
    "        X_test.requires_grad = False\n",
    "        if len(X_test.shape) ==3:\n",
    "            X_test = X_test.to(self.device)\n",
    "        else:\n",
    "            X_test = X_test.unsqueeze_(1).to(self.device)\n",
    "        \n",
    "        test_dataset = TensorDataset(X_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=int(min(X_test.shape[0] / 10, self.batch_size)), shuffle=False)\n",
    "        \n",
    "        self.OS_CNN.eval()\n",
    "        \n",
    "        predict_list = np.array([])\n",
    "        for sample in test_loader:\n",
    "            y_predict = self.OS_CNN(sample[0])\n",
    "            y_predict = y_predict.detach().cpu().numpy()\n",
    "            y_predict = np.argmax(y_predict, axis=1)\n",
    "            predict_list = np.concatenate((predict_list, y_predict), axis=0)\n",
    "            \n",
    "        return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_data(a,sampling):\n",
    "    a = np.float32(a)\n",
    "    a_torch = torch.from_numpy(a)\n",
    "    a_torch.unsqueeze_(1)\n",
    "    m = nn.MaxPool1d(sampling, stride=sampling)\n",
    "    output = m(a_torch)\n",
    "    return output.squeeze_(1).numpy()\n",
    "\n",
    "def down_sampling_data(a,sampling):\n",
    "    return a[:,::sampling]\n",
    "\n",
    "def long_data_to_more_channels(a, channels):\n",
    "    more_length_required = channels - a.shape[-1]%channels\n",
    "    padding_size = list(a.shape)\n",
    "    padding_size[-1]= int(more_length_required)\n",
    "    padding = np.zeros(tuple(padding_size))\n",
    "    a = np.concatenate((a, padding), axis=-1)\n",
    "    result = 1\n",
    "    for i in a.shape[1:]:\n",
    "        result = result*i\n",
    "    result = result*channels/a.shape[-1]\n",
    "    z = np.swapaxes(np.reshape(a,(a.shape[0],int(a.shape[-1]/channels),int(result))),1,2)\n",
    "    return np.float32(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = [\n",
    "'ArticularyWordRecognition',\n",
    "# 'AtrialFibrillation',\n",
    "# 'BasicMotions',\n",
    "# 'CharacterTrajectories',\n",
    "# 'Cricket',\n",
    "# 'DuckDuckGeese',\n",
    "# 'EigenWorms', \n",
    "# 'Epilepsy',\n",
    "# 'ERing',\n",
    "# 'EthanolConcentration',\n",
    "# 'FaceDetection',\n",
    "# 'FingerMovements',\n",
    "# 'HandMovementDirection',\n",
    "# 'Handwriting',\n",
    "# 'Heartbeat',\n",
    "# 'InsectWingbeat',# to long to run\n",
    "# 'JapaneseVowels',\n",
    "# 'Libras',\n",
    "# 'LSST',\n",
    "# 'MotorImagery',\n",
    "# 'NATOPS',\n",
    "# 'PEMS-SF',\n",
    "# 'PenDigits',\n",
    "# 'PhonemeSpectra',\n",
    "# 'RacketSports',\n",
    "# 'SelfRegulationSCP1',\n",
    "# 'SelfRegulationSCP2',\n",
    "# 'SpokenArabicDigits',\n",
    "# 'StandWalkJump',\n",
    "# 'UWaveGestureLibrary'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running at: ArticularyWordRecognition\n",
      "(275, 9, 144)\n",
      "code is running on  cuda:0\n",
      "[[(1, 6, 1), (1, 6, 2), (1, 6, 3), (1, 6, 5), (1, 6, 7), (1, 6, 11), (1, 6, 13), (1, 6, 17), (1, 6, 19), (1, 6, 23), (1, 6, 29), (1, 6, 31)], [(72, 9, 1), (72, 9, 2), (72, 9, 3), (72, 9, 5), (72, 9, 7), (72, 9, 11), (72, 9, 13), (72, 9, 17), (72, 9, 19), (72, 9, 23), (72, 9, 29), (72, 9, 31)], [(108, 72, 1), (108, 72, 2)]]\n",
      "os_block_layer_parameter_list is : [[(1, 6, 1), (1, 6, 2), (1, 6, 3), (1, 6, 5), (1, 6, 7), (1, 6, 11), (1, 6, 13), (1, 6, 17), (1, 6, 19), (1, 6, 23), (1, 6, 29), (1, 6, 31)], [(72, 9, 1), (72, 9, 2), (72, 9, 3), (72, 9, 5), (72, 9, 7), (72, 9, 11), (72, 9, 13), (72, 9, 17), (72, 9, 19), (72, 9, 23), (72, 9, 29), (72, 9, 31)]]\n",
      "in_channel_we_want is : 972\n",
      "[[(972, 6, 1), (972, 6, 2), (972, 6, 3), (972, 6, 5), (972, 6, 7), (972, 6, 11), (972, 6, 13), (972, 6, 17), (972, 6, 19), (972, 6, 23), (972, 6, 29), (972, 6, 31)], [(72, 9, 1), (72, 9, 2), (72, 9, 3), (72, 9, 5), (72, 9, 7), (72, 9, 11), (72, 9, 13), (72, 9, 17), (72, 9, 19), (72, 9, 23), (72, 9, 29), (72, 9, 31)], [(108, 72, 1), (108, 72, 2)]]\n",
      "16\n",
      "epoch = 49 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9833333333333333 \t loss=\t 1.370015025138855\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/ArticularyWordRecognition/ArticularyWordRecognition_.txt\n",
      "epoch = 99 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9866666666666667 \t loss=\t 0.02872721292078495\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/ArticularyWordRecognition/ArticularyWordRecognition_.txt\n",
      "correct: [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 17 17\n",
      " 18 18 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19 19 19\n",
      " 20 20 20 20 20 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 21 21 21 21\n",
      " 22 22 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 24 24 24 24 24 24 24 24 24 24 24 24]\n",
      "predict: [ 0. 20.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.\n",
      "  7.  7.  7.  7.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.  8. 10.  8.  8.\n",
      "  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 11. 18. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.\n",
      " 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n",
      " 13. 13. 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14.  1. 14. 14. 14.\n",
      " 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17. 17.\n",
      " 18. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18. 19. 19. 19. 19. 19. 19.\n",
      " 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.\n",
      " 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n",
      " 22. 22. 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23.\n",
      " 24. 24. 24. 24. 24. 24. 24. 24. 24. 24. 24. 24.]\n",
      "0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "Result_log_folder = './Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/'\n",
    "dataset_path = dirname(\"./Example_Datasets/UEAArchive_2018/\")\n",
    "\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    print('running at:', dataset_name)   \n",
    "    # load data\n",
    "    X_train, y_train, X_test, y_test = TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "    print(X_train.shape)\n",
    "    model = OS_CNN_easy_use(\n",
    "        Result_log_folder = Result_log_folder, # the Result_log_folder\n",
    "        dataset_name = dataset_name,           # dataset_name for creat log under Result_log_folder\n",
    "        device = \"cuda:0\",                     # Gpu \n",
    "        max_epoch = 100,                       # In our expirement the number is 2000 for keep it same with FCN for the example dataset 500 will be enough\n",
    "        Max_kernel_size = 89, \n",
    "        start_kernel_size = 1,\n",
    "        paramenter_number_of_layer_list = [8*128, (5*128*256 + 2*256*128)/2], \n",
    "        quarter_or_half = 4,\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_predict = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print('correct:',y_test)\n",
    "    print('predict:',y_predict)\n",
    "    acc = accuracy_score(y_predict, y_test)\n",
    "    print(acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
