{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import dirname\n",
    "\n",
    "\n",
    "def replace_nan_with_row_mean(a):\n",
    "    out = np.where(np.isnan(a), ma.array(a, mask=np.isnan(a)).mean(axis=1)[:, np.newaxis], a)\n",
    "    return np.float32(out)\n",
    "\n",
    "def replace_nan_with_near_value(a):\n",
    "    mask = np.isnan(a)\n",
    "    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
    "    np.maximum.accumulate(idx,axis=1, out=idx)\n",
    "    out = a[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return np.float32(out)\n",
    "\n",
    "def set_nan_to_zero(a):\n",
    "    where_are_NaNs = np.isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSC_data_loader(dataset_path,dataset_name):\n",
    "    Train_dataset = np.loadtxt(\n",
    "        dataset_path + '/' + dataset_name + '/' + dataset_name + '_TRAIN.tsv')\n",
    "    Test_dataset = np.loadtxt(\n",
    "        dataset_path + '/' + dataset_name + '/' + dataset_name + '_TEST.tsv')\n",
    "    Train_dataset = Train_dataset.astype(np.float32)\n",
    "    Test_dataset = Test_dataset.astype(np.float32)\n",
    "\n",
    "    X_train = Train_dataset[:, 1:]\n",
    "    y_train = Train_dataset[:, 0:1]\n",
    "\n",
    "    X_test = Test_dataset[:, 1:]\n",
    "    y_test = Test_dataset[:, 0:1]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.squeeze(y_train, axis=1))\n",
    "    y_train = le.transform(np.squeeze(y_train, axis=1))\n",
    "    y_test = le.transform(np.squeeze(y_test, axis=1))\n",
    "    return replace_nan_with_row_mean(X_train), y_train, replace_nan_with_row_mean(X_test), y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1422, 3, 180)\n",
      "(1422,)\n",
      "(1436, 3, 182)\n",
      "(1436,)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "def fill_out_with_Nan(data,max_length):\n",
    "    #via this it can works on more dimensional array\n",
    "    pad_length = max_length-data.shape[-1]\n",
    "    if pad_length == 0:\n",
    "        return data\n",
    "    else:\n",
    "        pad_shape = list(data.shape[:-1])\n",
    "        pad_shape.append(pad_length)\n",
    "        Nan_pad = np.empty(pad_shape)*np.nan\n",
    "        return np.concatenate((data, Nan_pad), axis=-1)\n",
    "    \n",
    "\n",
    "def get_label_dict(file_path):\n",
    "    label_dict ={}\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if '@classLabel' in line:\n",
    "                label_list = line.replace('\\n','').split(' ')[2:]\n",
    "                for i in range(len(label_list)):\n",
    "                    label_dict[label_list[i]] = i \n",
    "                \n",
    "                break\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def get_data_and_label_from_ts_file(file_path,label_dict):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        Start_reading_data = False\n",
    "        Label_list = []\n",
    "        Data_list = []\n",
    "        max_length = 0\n",
    "        for line in lines:\n",
    "            if Start_reading_data == False:\n",
    "                if '@data'in line:\n",
    "                    Start_reading_data = True\n",
    "            else:\n",
    "                temp = line.split(':')\n",
    "                Label_list.append(label_dict[temp[-1].replace('\\n','')])\n",
    "                data_tuple= [np.expand_dims(np.fromstring(channel, sep=','), axis=0) for channel in temp[:-1]]\n",
    "                max_channel_length = 0\n",
    "                for channel_data in data_tuple:\n",
    "                    if channel_data.shape[-1]>max_channel_length:\n",
    "                        max_channel_length = channel_data.shape[-1]\n",
    "                data_tuple = [fill_out_with_Nan(data,max_channel_length) for data in data_tuple]\n",
    "                data = np.expand_dims(np.concatenate(data_tuple, axis=0), axis=0)\n",
    "                Data_list.append(data)\n",
    "                if max_channel_length>max_length:\n",
    "                    max_length = max_channel_length\n",
    "        \n",
    "        Data_list = [fill_out_with_Nan(data,max_length) for data in Data_list]\n",
    "        X =  np.concatenate(Data_list, axis=0)\n",
    "        Y =  np.asarray(Label_list)\n",
    "        \n",
    "        return np.float32(X), Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def TSC_multivariate_data_loader(dataset_path, dataset_name):\n",
    "    \n",
    "    Train_dataset_path = dataset_path + '/' + dataset_name + '/' + dataset_name + '_TRAIN.ts'\n",
    "    Test_dataset_path = dataset_path + '/' + dataset_name + '/' + dataset_name + '_TEST.ts'\n",
    "    label_dict = get_label_dict(Train_dataset_path)\n",
    "    X_train, y_train = get_data_and_label_from_ts_file(Train_dataset_path,label_dict)\n",
    "    X_test, y_test = get_data_and_label_from_ts_file(Test_dataset_path,label_dict)\n",
    "    \n",
    "    return set_nan_to_zero(X_train), y_train, set_nan_to_zero(X_test), y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_name = 'CharacterTrajectories'\n",
    "dataset_path = dirname(\"./Datasets/UEAArchive_2018/\")\n",
    "#TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "\n",
    "X_train, y_train, X_test, y_test = TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_condition(iepoch,print_result_every_x_epoch):\n",
    "    if (iepoch + 1) % print_result_every_x_epoch == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    predict_list = np.array([])\n",
    "    label_list = np.array([])\n",
    "    for sample in dataloader:\n",
    "        y_predict = model(sample[0])\n",
    "        y_predict = y_predict.detach().cpu().numpy()\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "        predict_list = np.concatenate((predict_list, y_predict), axis=0)\n",
    "        label_list = np.concatenate((label_list, sample[1].detach().cpu().numpy()), axis=0)\n",
    "    acc = accuracy_score(predict_list, label_list)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def save_to_log(sentence, Result_log_folder, dataset_name):\n",
    "    father_path = Result_log_folder + dataset_name\n",
    "    if not os.path.exists(father_path):\n",
    "        os.makedirs(father_path)\n",
    "    path = father_path + '/' + dataset_name + '_.txt'\n",
    "    print(path)\n",
    "    with open(path, \"a\") as myfile:\n",
    "        myfile.write(sentence + '\\n')\n",
    "        \n",
    "        \n",
    "def get_Prime_number_in_a_range(start, end):\n",
    "    Prime_list = []\n",
    "    for val in range(start, end + 1): \n",
    "        prime_or_not = True\n",
    "        for n in range(2, val):\n",
    "            if (val % n) == 0:\n",
    "                prime_or_not = False\n",
    "                break\n",
    "        if prime_or_not:\n",
    "            Prime_list.append(val)\n",
    "    return Prime_list\n",
    "\n",
    "\n",
    "def get_out_channel_number(paramenter_layer, in_channel, prime_list):\n",
    "    out_channel_expect = int(paramenter_layer/(in_channel*sum(prime_list)))\n",
    "    return out_channel_expect\n",
    "\n",
    "def generate_layer_parameter_list(start,end,paramenter_number_of_layer_list, in_channel = 1):\n",
    "    prime_list = get_Prime_number_in_a_range(start, end)\n",
    "    if prime_list == []:\n",
    "        print('start = ',start, 'which is larger than end = ', end)\n",
    "    paramenter_number_of_layer_list[0] =  paramenter_number_of_layer_list[0]*in_channel\n",
    "    input_in_channel = in_channel\n",
    "    layer_parameter_list = []\n",
    "    for paramenter_number_of_layer in paramenter_number_of_layer_list:\n",
    "        out_channel = get_out_channel_number(paramenter_number_of_layer, in_channel, prime_list)\n",
    "        \n",
    "        tuples_in_layer= []\n",
    "        for prime in prime_list:\n",
    "            tuples_in_layer.append((in_channel,out_channel,prime))\n",
    "        in_channel =  len(prime_list)*out_channel\n",
    "        \n",
    "        layer_parameter_list.append(tuples_in_layer)\n",
    "    \n",
    "    tuples_in_layer_last = []\n",
    "    first_out_channel = len(prime_list)*get_out_channel_number(paramenter_number_of_layer_list[0], input_in_channel, prime_list)\n",
    "    tuples_in_layer_last.append((in_channel,first_out_channel,start))\n",
    "    tuples_in_layer_last.append((in_channel,first_out_channel,start+1))\n",
    "    layer_parameter_list.append(tuples_in_layer_last)\n",
    "    return layer_parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def calculate_mask_index(kernel_length_now,largest_kernel_lenght):\n",
    "    right_zero_mast_length = math.ceil((largest_kernel_lenght-1)/2)-math.ceil((kernel_length_now-1)/2)\n",
    "    left_zero_mask_length = largest_kernel_lenght - kernel_length_now - right_zero_mast_length\n",
    "    return left_zero_mask_length, left_zero_mask_length+ kernel_length_now\n",
    "\n",
    "def creat_mask(number_of_input_channel,number_of_output_channel, kernel_length_now, largest_kernel_lenght):\n",
    "    ind_left, ind_right= calculate_mask_index(kernel_length_now,largest_kernel_lenght)\n",
    "    mask = np.ones((number_of_input_channel,number_of_output_channel,largest_kernel_lenght))\n",
    "    mask[:,:,0:ind_left]=0\n",
    "    mask[:,:,ind_right:]=0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def creak_layer_mask(layer_parameter_list):\n",
    "    largest_kernel_lenght = layer_parameter_list[-1][-1]\n",
    "    mask_list = []\n",
    "    init_weight_list = []\n",
    "    bias_list = []\n",
    "    for i in layer_parameter_list:\n",
    "        conv = torch.nn.Conv1d(in_channels=i[0], out_channels=i[1], kernel_size=i[2])\n",
    "        ind_l,ind_r= calculate_mask_index(i[2],largest_kernel_lenght)\n",
    "        big_weight = np.zeros((i[1],i[0],largest_kernel_lenght))\n",
    "        big_weight[:,:,ind_l:ind_r]= conv.weight.detach().numpy()\n",
    "        \n",
    "        bias_list.append(conv.bias.detach().numpy())\n",
    "        init_weight_list.append(big_weight)\n",
    "        \n",
    "        mask = creat_mask(i[1],i[0],i[2], largest_kernel_lenght)\n",
    "        mask_list.append(mask)\n",
    "        \n",
    "    mask = np.concatenate(mask_list, axis=0)\n",
    "    init_weight = np.concatenate(init_weight_list, axis=0)\n",
    "    init_bias = np.concatenate(bias_list, axis=0)\n",
    "    return mask.astype(np.float32), init_weight.astype(np.float32), init_bias.astype(np.float32)\n",
    "\n",
    "    \n",
    "class build_layer_with_layer_parameter(nn.Module):\n",
    "    def __init__(self,layer_parameters):\n",
    "        super(build_layer_with_layer_parameter, self).__init__()\n",
    "\n",
    "        os_mask, init_weight, init_bias= creak_layer_mask(layer_parameters)\n",
    "        \n",
    "        \n",
    "        in_channels = os_mask.shape[1] \n",
    "        out_channels = os_mask.shape[0] \n",
    "        max_kernel_size = os_mask.shape[-1]\n",
    "\n",
    "        self.weight_mask = nn.Parameter(torch.from_numpy(os_mask),requires_grad=False)\n",
    "        \n",
    "        self.padding = nn.ConstantPad1d((int((max_kernel_size-1)/2), int(max_kernel_size/2)), 0)\n",
    "         \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=max_kernel_size)\n",
    "        self.conv1d.weight = nn.Parameter(torch.from_numpy(init_weight),requires_grad=True)\n",
    "        self.conv1d.bias =  nn.Parameter(torch.from_numpy(init_bias),requires_grad=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(num_features=out_channels)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.conv1d.weight.data = self.conv1d.weight*self.weight_mask\n",
    "        #self.conv1d.weight.data.mul_(self.weight_mask)\n",
    "        result_1 = self.padding(X)\n",
    "        result_2 = self.conv1d(result_1)\n",
    "        result_3 = self.bn(result_2)\n",
    "        result = F.relu(result_3)\n",
    "        return result    \n",
    "    \n",
    "    \n",
    "class OS_CNN_block(nn.Module):\n",
    "    def __init__(self,layer_parameter_list,n_class,squeeze_layer = True):\n",
    "        super(OS_CNN_block, self).__init__()\n",
    "        self.squeeze_layer = squeeze_layer\n",
    "        self.layer_parameter_list = layer_parameter_list\n",
    "        self.layer_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(layer_parameter_list)):\n",
    "            layer = build_layer_with_layer_parameter(layer_parameter_list[i])\n",
    "            self.layer_list.append(layer)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layer_list)\n",
    "            \n",
    "        self.averagepool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        out_put_channel_numebr = 0\n",
    "        for final_layer_parameters in layer_parameter_list[-1]:\n",
    "            out_put_channel_numebr = out_put_channel_numebr+ final_layer_parameters[1] \n",
    "            \n",
    "        self.hidden = nn.Linear(out_put_channel_numebr, n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.net(X)\n",
    "        \n",
    "        if self.squeeze_layer:\n",
    "            X = self.averagepool(X)\n",
    "            X = X.squeeze_(-1)\n",
    "            X = self.hidden(X)\n",
    "        return X\n",
    "\n",
    "def check_channel_limit(os_block_layer_parameter_list,n_input_channel,mid_channel_limit): \n",
    "    out_channel_each = 0\n",
    "    for conv_in in os_block_layer_parameter_list[-1]:\n",
    "        out_channel_each = out_channel_each + conv_in[1]\n",
    "    total_temp_channel = n_input_channel*out_channel_each\n",
    "    if total_temp_channel<=mid_channel_limit:\n",
    "        return os_block_layer_parameter_list\n",
    "    else:\n",
    "        \n",
    "        temp_channel_each = max(int(mid_channel_limit/(n_input_channel*len(os_block_layer_parameter_list[-1]))),1)\n",
    "        for i in range(len(os_block_layer_parameter_list[-1])):\n",
    "            os_block_layer_parameter_list[-1][i]= (os_block_layer_parameter_list[-1][i][0],\n",
    "                                                   temp_channel_each,\n",
    "                                                  os_block_layer_parameter_list[-1][i][2])\n",
    "        print('reshape temp channel from ',total_temp_channel,' to ',n_input_channel,' * ',temp_channel_each,)\n",
    "        return os_block_layer_parameter_list\n",
    "\n",
    "    \n",
    "class OS_CNN(nn.Module):\n",
    "    def __init__(self, layer_parameter_list, n_class, n_input_channel,squeeze_layer = True):\n",
    "        super(OS_CNN, self).__init__()\n",
    "        \n",
    "        self.mid_channel_limit = 1000\n",
    "        self.squeeze_layer = squeeze_layer\n",
    "        self.layer_parameter_list = layer_parameter_list\n",
    "        self.OS_block_list = nn.ModuleList()\n",
    "        \n",
    "        os_block_layer_parameter_list = copy.deepcopy(layer_parameter_list[:-1])\n",
    "        os_block_layer_parameter_list = check_channel_limit(os_block_layer_parameter_list,n_input_channel,self.mid_channel_limit)\n",
    "        print('os_block_layer_parameter_list is :',os_block_layer_parameter_list)\n",
    "        for nth in range(n_input_channel):\n",
    "            torch_OS_CNN_block = OS_CNN_block(os_block_layer_parameter_list,n_class, False)\n",
    "            self.OS_block_list.append(torch_OS_CNN_block)\n",
    "        \n",
    "        rf_size = layer_parameter_list[0][-1][-1]\n",
    "        in_channel_we_want= len(layer_parameter_list[1])*os_block_layer_parameter_list[-1][-1][1]*n_input_channel\n",
    "        print('in_channel_we_want is :', in_channel_we_want)\n",
    "       \n",
    "        layer_parameter_list = generate_layer_parameter_list(1,rf_size+1,[8*128, (5*128*256 + 2*256*128)/2],in_channel = in_channel_we_want)\n",
    "        \n",
    "        self.averagepool = nn.AdaptiveAvgPool1d(1) \n",
    "        print(layer_parameter_list)\n",
    "        self.OS_net =  OS_CNN_block(layer_parameter_list,n_class, True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        OS_block_result_list = []\n",
    "        for i_th_channel, OS_block in enumerate(self.OS_block_list):\n",
    "            OS_block_result = OS_block(X[:,i_th_channel:i_th_channel+1,:])\n",
    "            OS_block_result_list.append(OS_block_result)\n",
    "        result = F.relu(torch.cat(tuple(OS_block_result_list), 1)) \n",
    "        \n",
    "        result = self.OS_net(result)\n",
    "        return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import dirname\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "class OS_CNN_easy_use():\n",
    "    \n",
    "    def __init__(self,Result_log_folder, \n",
    "                 dataset_name, \n",
    "                 device, \n",
    "                 Max_kernel_size = 89, \n",
    "                 paramenter_number_of_layer_list = [8*128, 5*128*256 + 2*256*128], \n",
    "                 max_epoch = 2000, \n",
    "                 batch_size=16,\n",
    "                 print_result_every_x_epoch = 50,\n",
    "                 start_kernel_size = 1,\n",
    "                 quarter_or_half = 4\n",
    "                ):\n",
    "        \n",
    "        super(OS_CNN_easy_use, self).__init__()\n",
    "        \n",
    "        if not os.path.exists(Result_log_folder +dataset_name+'/'):\n",
    "            os.makedirs(Result_log_folder +dataset_name+'/')\n",
    "        Initial_model_path = Result_log_folder +dataset_name+'/'+dataset_name+'initial_model'\n",
    "        model_save_path = Result_log_folder +dataset_name+'/'+dataset_name+'Best_model'\n",
    "        \n",
    "\n",
    "        self.Result_log_folder = Result_log_folder\n",
    "        self.dataset_name = dataset_name        \n",
    "        self.model_save_path = model_save_path\n",
    "        self.Initial_model_path = Initial_model_path\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        \n",
    "        self.Max_kernel_size = Max_kernel_size\n",
    "        self.paramenter_number_of_layer_list = paramenter_number_of_layer_list\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.print_result_every_x_epoch = print_result_every_x_epoch\n",
    "        self.quarter_or_half = quarter_or_half\n",
    "        \n",
    "        self.OS_CNN = None\n",
    "        \n",
    "        \n",
    "        self.start_kernel_size = start_kernel_size\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "\n",
    "        print('code is running on ',self.device)\n",
    "        \n",
    "        # covert numpy to pytorch tensor and put into gpu\n",
    "        X_train = torch.from_numpy(X_train)\n",
    "        X_train.requires_grad = False\n",
    "        if len(X_train.shape) ==3:\n",
    "            X_train = X_train.to(self.device)\n",
    "        else:\n",
    "            X_train = X_train.unsqueeze_(1).to(self.device)\n",
    "        y_train = torch.from_numpy(y_train).to(self.device)\n",
    "        n_input_channel = X_train.shape[1]\n",
    "        \n",
    "        X_test = torch.from_numpy(X_val)\n",
    "        X_test.requires_grad = False\n",
    "        if len(X_test.shape) ==3:\n",
    "            X_test = X_test.to(self.device)\n",
    "        else:\n",
    "            X_test = X_test.unsqueeze_(1).to(self.device)\n",
    "        y_test = torch.from_numpy(y_val).to(self.device)\n",
    "        \n",
    "        \n",
    "        input_shape = X_train.shape[-1]\n",
    "        n_class = max(y_train) + 1\n",
    "        receptive_field_shape= min(int(X_train.shape[-1]/self.quarter_or_half),self.Max_kernel_size)\n",
    "        \n",
    "        # generate parameter list\n",
    "        layer_parameter_list = generate_layer_parameter_list(self.start_kernel_size,receptive_field_shape,self.paramenter_number_of_layer_list,in_channel = 1)\n",
    "        print(layer_parameter_list)\n",
    "        torch_OS_CNN = OS_CNN(layer_parameter_list, n_class.item(),n_input_channel, True).to(self.device)\n",
    "        \n",
    "        # save_initial_weight\n",
    "        torch.save(torch_OS_CNN.state_dict(), self.Initial_model_path)\n",
    "        \n",
    "        \n",
    "        # loss, optimizer, scheduler\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(torch_OS_CNN.parameters())\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=200, min_lr=0.0001)\n",
    "        \n",
    "        # build dataloader\n",
    "        print(max(int(min(X_train.shape[0] / 10, self.batch_size)),2))\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=max(int(min(X_train.shape[0] / 10, self.batch_size)),2), shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=max(int(min(X_train.shape[0] / 10, self.batch_size)),2), shuffle=False)\n",
    "        \n",
    "        \n",
    "        torch_OS_CNN.train()   \n",
    "        \n",
    "        for i in range(self.max_epoch):\n",
    "            for sample in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_predict = torch_OS_CNN(sample[0])\n",
    "                output = criterion(y_predict, sample[1])\n",
    "                output.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step(output)\n",
    "            \n",
    "            if eval_condition(i,self.print_result_every_x_epoch):\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print('epoch =',i, 'lr = ', param_group['lr'])\n",
    "                torch_OS_CNN.eval()\n",
    "                acc_train = eval_model(torch_OS_CNN, train_loader)\n",
    "                acc_test = eval_model(torch_OS_CNN, test_loader)\n",
    "                torch_OS_CNN.train()\n",
    "                print('train_acc=\\t', acc_train, '\\t test_acc=\\t', acc_test, '\\t loss=\\t', output.item())\n",
    "                sentence = 'train_acc=\\t'+str(acc_train)+ '\\t test_acc=\\t'+str(acc_test) \n",
    "                print('log saved at:')\n",
    "                save_to_log(sentence,self.Result_log_folder, self.dataset_name)\n",
    "                torch.save(torch_OS_CNN.state_dict(), self.model_save_path)\n",
    "         \n",
    "        torch.save(torch_OS_CNN.state_dict(), self.model_save_path)\n",
    "        self.OS_CNN = torch_OS_CNN\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        X_test = torch.from_numpy(X_test)\n",
    "        X_test.requires_grad = False\n",
    "        if len(X_test.shape) ==3:\n",
    "            X_test = X_test.to(self.device)\n",
    "        else:\n",
    "            X_test = X_test.unsqueeze_(1).to(self.device)\n",
    "        \n",
    "        test_dataset = TensorDataset(X_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=int(min(X_test.shape[0] / 10, self.batch_size)), shuffle=False)\n",
    "        \n",
    "        self.OS_CNN.eval()\n",
    "        \n",
    "        predict_list = np.array([])\n",
    "        for sample in test_loader:\n",
    "            y_predict = self.OS_CNN(sample[0])\n",
    "            y_predict = y_predict.detach().cpu().numpy()\n",
    "            y_predict = np.argmax(y_predict, axis=1)\n",
    "            predict_list = np.concatenate((predict_list, y_predict), axis=0)\n",
    "            \n",
    "        return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_data(a,sampling):\n",
    "    a = np.float32(a)\n",
    "    a_torch = torch.from_numpy(a)\n",
    "    a_torch.unsqueeze_(1)\n",
    "    m = nn.MaxPool1d(sampling, stride=sampling)\n",
    "    output = m(a_torch)\n",
    "    return output.squeeze_(1).numpy()\n",
    "\n",
    "def down_sampling_data(a,sampling):\n",
    "    return a[:,::sampling]\n",
    "\n",
    "def long_data_to_more_channels(a, channels):\n",
    "    more_length_required = channels - a.shape[-1]%channels\n",
    "    padding_size = list(a.shape)\n",
    "    padding_size[-1]= int(more_length_required)\n",
    "    padding = np.zeros(tuple(padding_size))\n",
    "    a = np.concatenate((a, padding), axis=-1)\n",
    "    result = 1\n",
    "    for i in a.shape[1:]:\n",
    "        result = result*i\n",
    "    result = result*channels/a.shape[-1]\n",
    "    z = np.swapaxes(np.reshape(a,(a.shape[0],int(a.shape[-1]/channels),int(result))),1,2)\n",
    "    return np.float32(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = [\n",
    "# 'ArticularyWordRecognition',\n",
    "# 'AtrialFibrillation',\n",
    "# 'BasicMotions',\n",
    "# 'CharacterTrajectories',\n",
    "# 'Cricket',\n",
    "# 'DuckDuckGeese',\n",
    "# 'EigenWorms', \n",
    "# 'Epilepsy',\n",
    "# 'ERing',\n",
    "# 'EthanolConcentration',\n",
    "# 'FaceDetection',\n",
    "# 'FingerMovements',\n",
    "# 'HandMovementDirection',\n",
    "# 'Handwriting',\n",
    "# 'Heartbeat',\n",
    "# 'InsectWingbeat',# to long to run\n",
    "'JapaneseVowels',# \n",
    "'Libras',\n",
    "'LSST',\n",
    "'MotorImagery', #out of memory\n",
    "'NATOPS',\n",
    "'PEMS-SF',\n",
    "'PenDigits',\n",
    "'PhonemeSpectra', #too long to run\n",
    "'RacketSports',\n",
    "'SelfRegulationSCP1',\n",
    "'SelfRegulationSCP2',\n",
    "'SpokenArabicDigits',# error\n",
    "'StandWalkJump',\n",
    "'UWaveGestureLibrary'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running at: JapaneseVowels\n",
      "(270, 12, 26)\n",
      "code is running on  cuda:0\n",
      "[[(1, 93, 1), (1, 93, 2), (1, 93, 3), (1, 93, 5)], [(372, 28, 1), (372, 28, 2), (372, 28, 3), (372, 28, 5)], [(112, 372, 1), (112, 372, 2)]]\n",
      "reshape temp channel from  1344  to  12  *  20\n",
      "os_block_layer_parameter_list is : [[(1, 93, 1), (1, 93, 2), (1, 93, 3), (1, 93, 5)], [(372, 20, 1), (372, 20, 2), (372, 20, 3), (372, 20, 5)]]\n",
      "in_channel_we_want is : 960\n",
      "[[(960, 93, 1), (960, 93, 2), (960, 93, 3), (960, 93, 5)], [(372, 28, 1), (372, 28, 2), (372, 28, 3), (372, 28, 5)], [(112, 372, 1), (112, 372, 2)]]\n",
      "16\n",
      "epoch = 49 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.981081081081081 \t loss=\t 0.0018114021513611078\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 99 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.00011157989501953125\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 149 lr =  0.001\n",
      "train_acc=\t 0.9925925925925926 \t test_acc=\t 0.9594594594594594 \t loss=\t 0.0013353824615478516\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 199 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 7.448877295246348e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 249 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9837837837837838 \t loss=\t 0.00014025824202690274\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 299 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9756756756756757 \t loss=\t 0.0033000707626342773\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 349 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 9.165491064777598e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 399 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.981081081081081 \t loss=\t 2.619198312459048e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 449 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9837837837837838 \t loss=\t 0.00030813898774795234\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 499 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9783783783783784 \t loss=\t 3.906658821506426e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 549 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.981081081081081 \t loss=\t 0.00023603439331054688\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 599 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9972972972972973 \t loss=\t 1.2261526762813446e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 649 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 6.338528328342363e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 699 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 0.0012935911072418094\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 749 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 3.746577590391098e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 799 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 8.44682926981477e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 849 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 8.276530934381299e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 899 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 6.130763381406723e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 949 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 3.984996055805823e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 999 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 1.3623919414840202e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1049 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 1.3623919414840202e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1099 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 4.495893335842993e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1149 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 1.8732888520389679e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1199 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 1.430511474609375e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1249 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 1.1580331147342804e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1299 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1349 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1399 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1449 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 2.5204249141097534e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1499 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 6.811959565311554e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1549 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 6.811959707420101e-08\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1599 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9864864864864865 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1649 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 2.0435878411717567e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1699 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1749 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1799 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9945945945945946 \t loss=\t 1.9754681943595642e-06\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1849 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 4.76837158203125e-07\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1899 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1949 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9891891891891892 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "epoch = 1999 lr =  0.0001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9918918918918919 \t loss=\t 0.0\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/JapaneseVowels/JapaneseVowels_.txt\n",
      "correct: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "predict: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 7. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 4. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6.\n",
      " 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6.\n",
      " 6. 6. 6. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
      " 8. 8. 4. 8. 8. 8. 8. 8. 8. 8.]\n",
      "0.9918918918918919\n",
      "running at: Libras\n",
      "(180, 2, 45)\n",
      "code is running on  cuda:0\n",
      "[[(1, 35, 1), (1, 35, 2), (1, 35, 3), (1, 35, 5), (1, 35, 7), (1, 35, 11)], [(210, 18, 1), (210, 18, 2), (210, 18, 3), (210, 18, 5), (210, 18, 7), (210, 18, 11)], [(108, 210, 1), (108, 210, 2)]]\n",
      "os_block_layer_parameter_list is : [[(1, 35, 1), (1, 35, 2), (1, 35, 3), (1, 35, 5), (1, 35, 7), (1, 35, 11)], [(210, 18, 1), (210, 18, 2), (210, 18, 3), (210, 18, 5), (210, 18, 7), (210, 18, 11)]]\n",
      "in_channel_we_want is : 216\n",
      "[[(216, 35, 1), (216, 35, 2), (216, 35, 3), (216, 35, 5), (216, 35, 7), (216, 35, 11)], [(210, 18, 1), (210, 18, 2), (210, 18, 3), (210, 18, 5), (210, 18, 7), (210, 18, 11)], [(108, 210, 1), (108, 210, 2)]]\n",
      "16\n",
      "epoch = 49 lr =  0.001\n",
      "train_acc=\t 0.9777777777777777 \t test_acc=\t 0.8611111111111112 \t loss=\t 0.34351712465286255\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 99 lr =  0.001\n",
      "train_acc=\t 0.9833333333333333 \t test_acc=\t 0.8888888888888888 \t loss=\t 0.5841798782348633\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 149 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9444444444444444 \t loss=\t 0.10004249215126038\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 199 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9222222222222223 \t loss=\t 1.0851562023162842\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 249 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9388888888888889 \t loss=\t 1.3518638610839844\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 299 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.95 \t loss=\t 0.1502116322517395\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 349 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9333333333333333 \t loss=\t 0.0269605815410614\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 399 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9444444444444444 \t loss=\t 0.04646545648574829\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 449 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.95 \t loss=\t 1.5184029340744019\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 499 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.3460581302642822\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 549 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.009599804878234863\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 599 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.18392285704612732\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 649 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 0.003016948699951172\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 699 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9388888888888889 \t loss=\t 0.00021588802337646484\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 749 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.95 \t loss=\t 0.024136781692504883\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 799 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9388888888888889 \t loss=\t 0.747465193271637\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 849 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9277777777777778 \t loss=\t 0.010769039392471313\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 899 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.1471528708934784\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 949 lr =  0.001\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.0005677938461303711\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 999 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9444444444444444 \t loss=\t 0.0006712973117828369\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1049 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9444444444444444 \t loss=\t 0.1237495094537735\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1099 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 0.08024758100509644\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1149 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9666666666666667 \t loss=\t 0.0003933906555175781\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1199 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 0.3644585609436035\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1249 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 0.22910793125629425\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1299 lr =  0.0005\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9444444444444444 \t loss=\t 0.0003705620765686035\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1349 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9666666666666667 \t loss=\t 0.02156001329421997\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1399 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 0.0005470514297485352\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1449 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9666666666666667 \t loss=\t 0.026145704090595245\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1499 lr =  0.00025\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9777777777777777 \t loss=\t 4.0650367736816406e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1549 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 0.0003173947334289551\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1599 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9777777777777777 \t loss=\t 0.00014847517013549805\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1649 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.95 \t loss=\t 8.565187454223633e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1699 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 0.00022453069686889648\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1749 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 3.814697265625e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1799 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9555555555555556 \t loss=\t 0.00011932849884033203\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1849 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 0.00026541948318481445\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1899 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 3.713369369506836e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1949 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9722222222222222 \t loss=\t 8.809566497802734e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "epoch = 1999 lr =  0.000125\n",
      "train_acc=\t 1.0 \t test_acc=\t 0.9611111111111111 \t loss=\t 4.553794860839844e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/Libras/Libras_.txt\n",
      "correct: [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14]\n",
      "predict: [ 0.  0. 12.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "  3.  3.  6.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4. 13.  4.  4.\n",
      "  4.  4.  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  3.  5.  5.  5.  5.  5.\n",
      "  6.  6.  6.  6.  6.  6.  6.  6.  6.  3.  6.  6.  7.  7.  7.  7.  7.  7.\n",
      "  7.  7.  7. 14.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "  9.  9. 12.  9.  9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.\n",
      " 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n",
      " 13. 13. 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14.]\n",
      "0.9611111111111111\n",
      "running at: LSST\n",
      "(2459, 6, 36)\n",
      "code is running on  cuda:0\n",
      "[[(1, 56, 1), (1, 56, 2), (1, 56, 3), (1, 56, 5), (1, 56, 7)], [(280, 22, 1), (280, 22, 2), (280, 22, 3), (280, 22, 5), (280, 22, 7)], [(110, 280, 1), (110, 280, 2)]]\n",
      "os_block_layer_parameter_list is : [[(1, 56, 1), (1, 56, 2), (1, 56, 3), (1, 56, 5), (1, 56, 7)], [(280, 22, 1), (280, 22, 2), (280, 22, 3), (280, 22, 5), (280, 22, 7)]]\n",
      "in_channel_we_want is : 660\n",
      "[[(660, 56, 1), (660, 56, 2), (660, 56, 3), (660, 56, 5), (660, 56, 7)], [(280, 22, 1), (280, 22, 2), (280, 22, 3), (280, 22, 5), (280, 22, 7)], [(110, 280, 1), (110, 280, 2)]]\n",
      "16\n",
      "epoch = 49 lr =  0.001\n",
      "train_acc=\t 0.3578690524603497 \t test_acc=\t 0.3304947283049473 \t loss=\t 0.7858350276947021\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 99 lr =  0.001\n",
      "train_acc=\t 0.41276941846278975 \t test_acc=\t 0.3386050283860503 \t loss=\t 0.1523457169532776\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 149 lr =  0.001\n",
      "train_acc=\t 0.4936966246441643 \t test_acc=\t 0.4768856447688564 \t loss=\t 0.10462986677885056\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 199 lr =  0.001\n",
      "train_acc=\t 0.5010166734444896 \t test_acc=\t 0.4286293592862936 \t loss=\t 0.044684216380119324\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 249 lr =  0.001\n",
      "train_acc=\t 0.4770231801545344 \t test_acc=\t 0.4067315490673155 \t loss=\t 0.038189616054296494\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 299 lr =  0.001\n",
      "train_acc=\t 0.5725904839365595 \t test_acc=\t 0.5117599351175993 \t loss=\t 0.009663327597081661\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 349 lr =  0.001\n",
      "train_acc=\t 0.24156161041073607 \t test_acc=\t 0.20437956204379562 \t loss=\t 0.3376857042312622\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 399 lr =  0.001\n",
      "train_acc=\t 0.3786091907279382 \t test_acc=\t 0.3288726682887267 \t loss=\t 0.08091812580823898\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 449 lr =  0.001\n",
      "train_acc=\t 0.3904026026840179 \t test_acc=\t 0.32725060827250607 \t loss=\t 3.7578018236672506e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 499 lr =  0.001\n",
      "train_acc=\t 0.2549816998779992 \t test_acc=\t 0.21654501216545013 \t loss=\t 0.003080806927755475\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 549 lr =  0.001\n",
      "train_acc=\t 0.4078893859292395 \t test_acc=\t 0.31346309813463097 \t loss=\t 0.013204357586801052\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 599 lr =  0.001\n",
      "train_acc=\t 0.5595770638470923 \t test_acc=\t 0.4894566098945661 \t loss=\t 0.010599561966955662\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 649 lr =  0.001\n",
      "train_acc=\t 0.8121187474583164 \t test_acc=\t 0.6062449310624494 \t loss=\t 0.007681210059672594\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 699 lr =  0.001\n",
      "train_acc=\t 0.6710044733631557 \t test_acc=\t 0.5381184103811841 \t loss=\t 0.001078984932973981\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 749 lr =  0.001\n",
      "train_acc=\t 0.5786905246034973 \t test_acc=\t 0.44363341443633414 \t loss=\t 5.037066512159072e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 799 lr =  0.001\n",
      "train_acc=\t 0.3204554697031314 \t test_acc=\t 0.24898621248986214 \t loss=\t 0.0020986010786145926\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 849 lr =  0.0005\n",
      "train_acc=\t 0.34648230988206585 \t test_acc=\t 0.2643957826439578 \t loss=\t 2.0135532395215705e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 899 lr =  0.0005\n",
      "train_acc=\t 0.49735664904432697 \t test_acc=\t 0.37550689375506896 \t loss=\t 0.00021021881548222154\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 949 lr =  0.0005\n",
      "train_acc=\t 0.7108580723871493 \t test_acc=\t 0.5587996755879967 \t loss=\t 0.000610936782322824\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 999 lr =  0.0005\n",
      "train_acc=\t 0.4884099227328182 \t test_acc=\t 0.3661800486618005 \t loss=\t 0.03489246219396591\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1049 lr =  0.0005\n",
      "train_acc=\t 0.6344042293615291 \t test_acc=\t 0.5466342254663422 \t loss=\t 5.525892265723087e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1099 lr =  0.0005\n",
      "train_acc=\t 0.37982919886132577 \t test_acc=\t 0.2935928629359286 \t loss=\t 0.4771367907524109\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1149 lr =  0.00025\n",
      "train_acc=\t 0.5315168767791786 \t test_acc=\t 0.4781021897810219 \t loss=\t 5.06097603647504e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1199 lr =  0.00025\n",
      "train_acc=\t 0.857259048393656 \t test_acc=\t 0.6038118410381184 \t loss=\t 6.803599535487592e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1249 lr =  0.00025\n",
      "train_acc=\t 0.32492883285888574 \t test_acc=\t 0.22546634225466342 \t loss=\t 0.00019981102377641946\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1299 lr =  0.00025\n",
      "train_acc=\t 0.2659617730784872 \t test_acc=\t 0.20316301703163017 \t loss=\t 4.2384319385746494e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1349 lr =  0.00025\n",
      "train_acc=\t 0.8064253761691744 \t test_acc=\t 0.5815085158150851 \t loss=\t 0.00025965951499529183\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n",
      "epoch = 1399 lr =  0.00025\n",
      "train_acc=\t 0.43391622610817404 \t test_acc=\t 0.3523925385239254 \t loss=\t 2.4101951566990465e-05\n",
      "log saved at:\n",
      "./Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/LSST/LSST_.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "Result_log_folder = './Results_of_OS_OS_CNN_for_UEAArchive_2018/OS_CNN_result_iter_0/'\n",
    "dataset_path = dirname(\"./Datasets/UEAArchive_2018/\")\n",
    "\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    print('running at:', dataset_name)   \n",
    "    # load data\n",
    "    X_train, y_train, X_test, y_test = TSC_multivariate_data_loader(dataset_path, dataset_name)\n",
    "    print(X_train.shape)\n",
    "    model = OS_CNN_easy_use(\n",
    "        Result_log_folder = Result_log_folder, # the Result_log_folder\n",
    "        dataset_name = dataset_name,           # dataset_name for creat log under Result_log_folder\n",
    "        device = \"cuda:0\",                     # Gpu \n",
    "        max_epoch = 2000,                       # In our expirement the number is 2000 for keep it same with FCN for the example dataset 500 will be enough\n",
    "        Max_kernel_size = 89, \n",
    "        start_kernel_size = 1,\n",
    "        paramenter_number_of_layer_list = [8*128, (5*128*256 + 2*256*128)/2], \n",
    "        quarter_or_half = 4,\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_predict = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print('correct:',y_test)\n",
    "    print('predict:',y_predict)\n",
    "    acc = accuracy_score(y_predict, y_test)\n",
    "    print(acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
